{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CHAPTER 2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BPE tokenizer"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![image.png](c2/1.png)"
  },
  {
   "metadata": {
    "id": "E_Yt5crGXyXZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dfc25d98-c4f5-436a-e9d0-eda05872d9d4"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n"
     ]
    }
   ],
   "execution_count": null,
   "source": "pip install tiktoken"
  },
  {
   "metadata": {
    "id": "akvyTdfQXyUV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3f4e0c29-4467-4536-f79a-f05596fc3e4b"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version:  0.9.0\n"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version: \", version(\"tiktoken\"))"
   ]
  },
  {
   "metadata": {
    "id": "czxWWLnkXyRl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8cd071e7-011f-4dbd-a302-3bd7bf5d2364"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "BPE_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "tokens = BPE_tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "id": "0ul381moXyOp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2940c485-0c02-4141-c682-98d65af955df"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "decode_text = BPE_tokenizer.decode(tokens)\n",
    "print(decode_text)"
   ]
  },
  {
   "metadata": {
    "id": "RaKvG4dJXyL2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ad5735c0-3f42-4768-ddcb-cfea4e604d62"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33901: Ak\n",
      "86: w\n",
      "343: ir\n",
      "86: w\n",
      "220:  \n",
      "959: ier\n"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "unknow_word = \"Akwirw ier\"\n",
    "encode_nums = BPE_tokenizer.encode(unknow_word)\n",
    "for num in encode_nums:\n",
    "    print(f\"{num}: {BPE_tokenizer.decode([num])}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![image.png](c2/2.png)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![image.png](c2/3.png)"
  },
  {
   "metadata": {
    "id": "hlpbQeKzXyI6"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n"
   ]
  },
  {
   "metadata": {
    "id": "U5Tkzhj7XyGB"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class GPTdatasetV1(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Dataset for GPT-2, Slide Window.\n",
    "    txt: Input text.\n",
    "    tokenizer: Tokenizer.\n",
    "    window_length: Length of the window.\n",
    "    stride: Stride of the window. (How many words to skip).\n",
    "\n",
    "    __len__: return size of the input data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, txt, tokenizer, window_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.output_ids = []\n",
    "\n",
    "        words_to_nums = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(words_to_nums) - window_length, stride):\n",
    "            self.input_ids.append(torch.tensor(words_to_nums[i:i+window_length]))\n",
    "            self.output_ids.append(torch.tensor(words_to_nums[i+1:i+window_length+1]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.output_ids[idx]\n"
   ]
  },
  {
   "metadata": {
    "id": "mzLddUvfXyCo"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Customize dataloader\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, window_length=256, stride=128,shuffle=True, drop_last=True, num_workers=0):\n",
    "    # define tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # define dataset\n",
    "    dataset = GPTdatasetV1(txt, tokenizer, window_length, stride)\n",
    "\n",
    "    # define dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDafkHjSfzbt",
    "outputId": "b8d7b032-31f5-4698-87ec-7d4ab9932cce"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x7e1534767090>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMtpcgRHepZv",
    "outputId": "51ce84f6-e26c-4f7d-b50d-3e1a4593e270"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "with open(\"the-verdict.txt\", \"r\") as fn:\n",
    "    raw_txt = fn.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_txt, 1, 4, 1, False)\n",
    "iter_dataloader = iter(dataloader)\n",
    "first = next(iter_dataloader)\n",
    "first"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Token Embedding"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![image.png](c2/5.png)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##Encoding word positions"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![image.png](c2/6.png)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The way the previously introduced embedding layer works is that the same token ID always gets mapped to the same vector representation, regardless of where the token ID is positioned in the input sequence.The embedding layer converts a token ID into the same vector representation regardless of where it is located in the input sequence. For example, the token ID 5, whether it’s in the first or fourth position in the token ID input vector, will result in the same embedding vector."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "since the self-attention mechanism of LLMs itself is also position-agnostic, it is helpful to inject additional position information into the LLM."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##Absolute positional embeddings\n",
    "\n",
    "are directly associated with specific positions in a sequence. For each position in the input sequence, a unique embedding is added to the token’s embedding to convey its exact location. For instance, the first token will have a specific positional embedding, the second token another distinct embedding, and so on.\n",
    "\n",
    "![image.png](c2/7.png)\n",
    "\n",
    "Positional embeddings are added to the token embedding vector to create the input embeddings for an LLM. The positional vectors have the same dimension as the original token embeddings. The token embeddings are shown with value 1 for simplicity."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Relative positional embeddings\n",
    "\n",
    "is on the relative position or distance between tokens. This means the model learns the relationships in terms of “how far apart” rather than “at which exact position.” The advantage here is that the model can generalize better to sequences of varying lengths, even if it hasn’t seen such lengths during training."
   ]
  },
  {
   "metadata": {
    "id": "DqazgrfYBYjb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f12ab754-ac13-4ad3-8597-f2ba2082e705"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 256)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "token_embedding_layer"
   ]
  },
  {
   "metadata": {
    "id": "L6JfVy2EXx_S",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a3bb2a2b-c5e9-4408-e510-d998548f68ad"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "# Batch size of 8 with 4 tokens each, 8 * 4 * 256\n",
    "\n",
    "window_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, 8, window_length, window_length, shuffle=False)\n",
    "dataloader_iter = iter(dataloader)\n",
    "inputs, outputs = next(dataloader_iter)\n",
    "inputs.shape\n",
    "# 8 words input , 4 for each word"
   ]
  },
  {
   "metadata": {
    "id": "Oy1A4AyYd7tW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0d42a749-dfbd-4ca8-9768-ddc501b1492a"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "token_embedding = token_embedding_layer(inputs)\n",
    "token_embedding.shape"
   ]
  },
  {
   "metadata": {
    "id": "xdm4MSUod7R2"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# words\n",
    "#-> create a Class  (tensor dataset) return [1. input words ids, 2. predict the next words ids] with encoder your choosed\n",
    "#-> create dataloader with tiktoken encoder.return a tensor dataloader, [input, output(predict next word id)]\n",
    "#-> use torch.nn.Embedding(datasize, output_dim) create a position matrix\n",
    "#-> take the input from tensor dataloader apply to position matrix"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For a GPT model’s absolute embedding approach, we just need to create another embedding layer that has the same embedding dimension as the token_embedding_ layer:"
  },
  {
   "metadata": {
    "id": "gdcuW1Z6ah6X",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3463406d-afe6-4aa6-c194-c8ce08e24ec9"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5664, -0.3090,  1.0387,  ..., -1.4311, -1.0868, -0.3274],\n",
       "        [ 1.4174, -0.0903,  0.5353,  ...,  1.2204, -1.6243, -2.0949],\n",
       "        [ 0.0650, -0.6784,  0.0647,  ..., -0.2292, -0.2698,  0.0311],\n",
       "        [-0.4812,  0.1008,  1.2476,  ...,  0.7328,  0.7332, -0.8771]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "context_length = max_length = 4\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)\n",
    "pos_embeddings\n",
    "# Note: layer is changeable even adopted to other layers\n",
    "# pos_embedding_layer [changeable] -> torch.arange layer [static]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**torch.nn.Embedding(context_length, output_dim)**\n",
    "\n",
    "Creates a learnable lookup table (i.e. embedding matrix) of shape [context_length, output_dim].\n",
    "\n",
    "Each index i from 0 to context_length - 1 maps to a vector of size output_dim.\n",
    "\n",
    "**torch.arange(context_length)**\n",
    "\n",
    "Generates the sequence [0, 1, 2, ..., context_length - 1]. These are the absolute positions in the input sequence.\n",
    "\n",
    "**pos_embedding_layer(...)**\n",
    "\n",
    "Looks up the embedding for each absolute position index.\n",
    "\n",
    "So position 0 always maps to the same vector, position 1 to another, and so on.\n",
    "\n",
    "Result shape: [context_length, output_dim]\n",
    "\n",
    "One embedding vector per absolute position."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![image.png](c2/8.png)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SUMMARY"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Raw text is broken into tokens, [words or charactors]\n",
    "2. Tokens into integers -- token IDs\n",
    "3. Handle special tokens: <|unk|> and <|endoftext|>\n",
    "4. => BPE tokenizer -- tiktoken.get_encoding(\"gpt2\")\n",
    "5. Sliding window approach on tokenized data, input <-> predicted next word\n",
    "6. Embedding layers in PyTorch function as a lookup operation, retrieving vectors corresponding to token IDs.\n",
    "7. OpenAI’s GPT models utilize absolute positional embeddings, which are added to the token embedding vectors and are optimized during the model training."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Start Token Embedding"
  },
  {
   "metadata": {
    "id": "lonRWe96ahwe",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e5d169ac-afc3-457e-8d2a-186cc9c19fd7"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n"
     ]
    }
   ],
   "execution_count": null,
   "source": "pip install tiktoken"
  },
  {
   "metadata": {
    "id": "KuJ5GeK4uVNN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b443788e-8476-4f9a-82ce-d050624a750d"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2953, 428, 966, 11, 345]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "# 4. => BPE tokenizer -- tiktoken.get_encoding(\"gpt2\")\n",
    "import tiktoken\n",
    "raw_text = \"At this point, you know how to prepare the input text for training LLMs by splitting text into individual word and subword tokens, which can be encoded into vector representations, embeddings, for the LLM.\"\n",
    "BPE_tokenizer_v1 = tiktoken.get_encoding(\"gpt2\")\n",
    "txt_encoding = BPE_tokenizer_v1.encode(raw_text)\n",
    "txt_encoding[:5]"
   ]
  },
  {
   "metadata": {
    "id": "ks9aQWNWuVD4"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#5. Sliding window approach on tokenized data, input <-> predicted next word\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPT_Dataset(Dataset):\n",
    "  def __init__(self, txt, tokenizer, window_size, steps):\n",
    "    self.input_ids = []\n",
    "    self.output_ids = []\n",
    "\n",
    "    word_nums = tokenizer.encode(txt)\n",
    "    for i in range(0, len(word_nums) - window_size, steps):\n",
    "      tensor_input = torch.tensor(word_nums[i : i + window_size])\n",
    "      tensor_output = torch.tensor(word_nums[i + 1 : i + window_size + 1])\n",
    "      self.input_ids.append(tensor_input)\n",
    "      self.output_ids.append(tensor_output)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.output_ids[idx]\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "id": "fii3WHmnuVAu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e8d41f27-7fc1-43fb-e1b0-080aa2bb9d29"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "#Embedding layers in PyTorch function as a lookup operation, retrieving vectors corresponding to token IDs\n",
    "def customize_dataloader(txt, batch_size=4, window_size=256, steps=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "  dataset = GPT_Dataset(txt, tokenizer, window_size, steps)\n",
    "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "  return dataloader\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\") as fn:\n",
    "    raw_txt = fn.read()\n",
    "\n",
    "window_size = 4\n",
    "dataloader = customize_dataloader(raw_text, 8, window_size, window_size, shuffle=False)\n",
    "inputs, outputs = next(iter(dataloader))\n",
    "inputs.shape\n"
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d66f9174-b784-4ce7-dc31-cddf3fa597c8",
    "id": "mVf2f27-onww"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "# OpenAI’s GPT models utilize absolute positional embeddings, which are added to the token embedding vectors and are optimized during the model training.\n",
    "\n",
    "input_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(input_size, output_dim)\n",
    "token_embedding = token_embedding_layer(inputs)\n",
    "token_embedding.shape"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Start Positional Embeddings"
  },
  {
   "metadata": {
    "id": "h4kKiuyNuU9j",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "53f74c2d-9b13-462b-e7f5-8c8f66467288"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "context_length = windew_size = 4\n",
    "position_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "position_embedding = position_embedding_layer(torch.arange(context_length))\n",
    "position_embedding.shape"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Start Input Embedding"
  },
  {
   "metadata": {
    "id": "DaXr68cVuU6X",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "99d927a1-fa46-4197-877f-be9aaf967082"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "input_embedding = token_embedding + position_embedding\n",
    "input_embedding.shape"
   ]
  }
 ]
}
