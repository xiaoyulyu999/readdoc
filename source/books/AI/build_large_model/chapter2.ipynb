{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CHAPTER 2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BPE tokenizer"
  },
  {
   "metadata": {
    "id": "E_Yt5crGXyXZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dfc25d98-c4f5-436a-e9d0-eda05872d9d4",
    "ExecuteTime": {
     "end_time": "2025-06-23T11:38:54.408140Z",
     "start_time": "2025-06-23T11:38:53.388693Z"
    }
   },
   "cell_type": "code",
   "source": "pip install tiktoken",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/lyuxiaoyu/Desktop/readthedoc/.venv/lib/python3.12/site-packages (0.9.0)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/lyuxiaoyu/Desktop/readthedoc/.venv/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/lyuxiaoyu/Desktop/readthedoc/.venv/lib/python3.12/site-packages (from tiktoken) (2.32.4)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/lyuxiaoyu/Desktop/readthedoc/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lyuxiaoyu/Desktop/readthedoc/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lyuxiaoyu/Desktop/readthedoc/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lyuxiaoyu/Desktop/readthedoc/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "id": "akvyTdfQXyUV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3f4e0c29-4467-4536-f79a-f05596fc3e4b",
    "ExecuteTime": {
     "end_time": "2025-06-23T11:38:54.421538Z",
     "start_time": "2025-06-23T11:38:54.416170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version: \", version(\"tiktoken\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version:  0.9.0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "id": "czxWWLnkXyRl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8cd071e7-011f-4dbd-a302-3bd7bf5d2364",
    "ExecuteTime": {
     "end_time": "2025-06-23T11:38:54.458644Z",
     "start_time": "2025-06-23T11:38:54.456392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BPE_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "tokens = BPE_tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(tokens)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "id": "0ul381moXyOp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2940c485-0c02-4141-c682-98d65af955df",
    "ExecuteTime": {
     "end_time": "2025-06-23T11:39:12.223775Z",
     "start_time": "2025-06-23T11:39:12.221157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decode_text = BPE_tokenizer.decode(tokens)\n",
    "print(decode_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "id": "RaKvG4dJXyL2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ad5735c0-3f42-4768-ddcb-cfea4e604d62",
    "ExecuteTime": {
     "end_time": "2025-06-23T11:38:54.517034Z",
     "start_time": "2025-06-23T11:38:54.514361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unknow_word = \"Akwirw ier\"\n",
    "encode_nums = BPE_tokenizer.encode(unknow_word)\n",
    "for num in encode_nums:\n",
    "    print(f\"{num}: {BPE_tokenizer.decode([num])}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33901: Ak\n",
      "86: w\n",
      "343: ir\n",
      "86: w\n",
      "220:  \n",
      "959: ier\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "id": "hlpbQeKzXyI6",
    "ExecuteTime": {
     "end_time": "2025-06-23T11:38:58.286400Z",
     "start_time": "2025-06-23T11:38:54.542287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lyuxiaoyu/Desktop/readthedoc/.venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "id": "U5Tkzhj7XyGB",
    "ExecuteTime": {
     "end_time": "2025-06-23T11:39:02.826248Z",
     "start_time": "2025-06-23T11:39:02.822848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTdatasetV1(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Dataset for GPT-2, Slide Window.\n",
    "    txt: Input text.\n",
    "    tokenizer: Tokenizer.\n",
    "    window_length: Length of the window.\n",
    "    stride: Stride of the window. (How many words to skip).\n",
    "\n",
    "    __len__: return size of the input data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, txt, tokenizer, window_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.output_ids = []\n",
    "\n",
    "        words_to_nums = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(words_to_nums) - window_length, stride):\n",
    "            self.input_ids.append(torch.tensor(words_to_nums[i:i+window_length]))\n",
    "            self.output_ids.append(torch.tensor(words_to_nums[i+1:i+window_length+1]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.output_ids[idx]\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "id": "mzLddUvfXyCo",
    "ExecuteTime": {
     "end_time": "2025-06-23T11:39:13.943207Z",
     "start_time": "2025-06-23T11:39:13.940965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Customize dataloader\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, window_length=256, stride=128,shuffle=True, drop_last=True, num_workers=0):\n",
    "    # define tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # define dataset\n",
    "    dataset = GPTdatasetV1(txt, tokenizer, window_length, stride)\n",
    "\n",
    "    # define dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDafkHjSfzbt",
    "outputId": "b8d7b032-31f5-4698-87ec-7d4ab9932cce",
    "ExecuteTime": {
     "end_time": "2025-06-23T11:39:17.396714Z",
     "start_time": "2025-06-23T11:39:17.215934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ],
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)>",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mSSLCertVerificationError\u001B[39m                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/fbcode/platform010/Python3.12.framework/Versions/3.12/lib/python3.12/urllib/request.py:1344\u001B[39m, in \u001B[36mAbstractHTTPHandler.do_open\u001B[39m\u001B[34m(self, http_class, req, **http_conn_args)\u001B[39m\n\u001B[32m   1343\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1344\u001B[39m     \u001B[43mh\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mselector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1345\u001B[39m \u001B[43m              \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhas_header\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mTransfer-encoding\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1346\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/fbcode/platform010/Python3.12.framework/Versions/3.12/lib/python3.12/http/client.py:1338\u001B[39m, in \u001B[36mHTTPConnection.request\u001B[39m\u001B[34m(self, method, url, body, headers, encode_chunked)\u001B[39m\n\u001B[32m   1337\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1338\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/fbcode/platform010/Python3.12.framework/Versions/3.12/lib/python3.12/http/client.py:1384\u001B[39m, in \u001B[36mHTTPConnection._send_request\u001B[39m\u001B[34m(self, method, url, body, headers, encode_chunked)\u001B[39m\n\u001B[32m   1383\u001B[39m     body = _encode(body, \u001B[33m'\u001B[39m\u001B[33mbody\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1384\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mendheaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/fbcode/platform010/Python3.12.framework/Versions/3.12/lib/python3.12/http/client.py:1333\u001B[39m, in \u001B[36mHTTPConnection.endheaders\u001B[39m\u001B[34m(self, message_body, encode_chunked)\u001B[39m\n\u001B[32m   1332\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m CannotSendHeader()\n\u001B[32m-> \u001B[39m\u001B[32m1333\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/fbcode/platform010/Python3.12.framework/Versions/3.12/lib/python3.12/http/client.py:1093\u001B[39m, in \u001B[36mHTTPConnection._send_output\u001B[39m\u001B[34m(self, message_body, encode_chunked)\u001B[39m\n\u001B[32m   1092\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m._buffer[:]\n\u001B[32m-> \u001B[39m\u001B[32m1093\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1095\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m message_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1096\u001B[39m \n\u001B[32m   1097\u001B[39m     \u001B[38;5;66;03m# create a consistent interface to message_body\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/fbcode/platform010/Python3.12.framework/Versions/3.12/lib/python3.12/http/client.py:1037\u001B[39m, in \u001B[36mHTTPConnection.send\u001B[39m\u001B[34m(self, data)\u001B[39m\n\u001B[32m   1036\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.auto_open:\n\u001B[32m-> \u001B[39m\u001B[32m1037\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1038\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/fbcode/platform010/Python3.12.framework/Versions/3.12/lib/python3.12/http/client.py:1479\u001B[39m, in \u001B[36mHTTPSConnection.connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1477\u001B[39m     server_hostname = \u001B[38;5;28mself\u001B[39m.host\n\u001B[32m-> \u001B[39m\u001B[32m1479\u001B[39m \u001B[38;5;28mself\u001B[39m.sock = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_context\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrap_socket\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1480\u001B[39m \u001B[43m                                      \u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[43m=\u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/fbcode/platform010/Python3.12.framework/Versions/3.12/lib/python3.12/ssl.py:455\u001B[39m, in \u001B[36mSSLContext.wrap_socket\u001B[39m\u001B[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001B[39m\n\u001B[32m    449\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrap_socket\u001B[39m(\u001B[38;5;28mself\u001B[39m, sock, server_side=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    450\u001B[39m                 do_handshake_on_connect=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    451\u001B[39m                 suppress_ragged_eofs=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    452\u001B[39m                 server_hostname=\u001B[38;5;28;01mNone\u001B[39;00m, session=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    453\u001B[39m     \u001B[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001B[39;00m\n\u001B[32m    454\u001B[39m     \u001B[38;5;66;03m# ctx._wrap_socket()\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m455\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msslsocket_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_create\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    456\u001B[39m \u001B[43m        \u001B[49m\u001B[43msock\u001B[49m\u001B[43m=\u001B[49m\u001B[43msock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    457\u001B[39m \u001B[43m        \u001B[49m\u001B[43mserver_side\u001B[49m\u001B[43m=\u001B[49m\u001B[43mserver_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    458\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdo_handshake_on_connect\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdo_handshake_on_connect\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    459\u001B[39m \u001B[43m        \u001B[49m\u001B[43msuppress_ragged_eofs\u001B[49m\u001B[43m=\u001B[49m\u001B[43msuppress_ragged_eofs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    460\u001B[39m \u001B[43m        \u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[43m=\u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    461\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    462\u001B[39m \u001B[43m        \u001B[49m\u001B[43msession\u001B[49m\u001B[43m=\u001B[49m\u001B[43msession\u001B[49m\n\u001B[32m    463\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/fbcode/platform010/Python3.12.framework/Versions/3.12/lib/python3.12/ssl.py:1041\u001B[39m, in \u001B[36mSSLSocket._create\u001B[39m\u001B[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001B[39m\n\u001B[32m   1040\u001B[39m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1041\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdo_handshake\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1042\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/fbcode/platform010/Python3.12.framework/Versions/3.12/lib/python3.12/ssl.py:1319\u001B[39m, in \u001B[36mSSLSocket.do_handshake\u001B[39m\u001B[34m(self, block)\u001B[39m\n\u001B[32m   1318\u001B[39m         \u001B[38;5;28mself\u001B[39m.settimeout(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m-> \u001B[39m\u001B[32m1319\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sslobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdo_handshake\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1320\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[31mSSLCertVerificationError\u001B[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mURLError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      2\u001B[39m url = (\u001B[33m\"\u001B[39m\u001B[33mhttps://raw.githubusercontent.com/rasbt/\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      3\u001B[39m        \u001B[33m\"\u001B[39m\u001B[33mLLMs-from-scratch/main/ch02/01_main-chapter-code/\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      4\u001B[39m        \u001B[33m\"\u001B[39m\u001B[33mthe-verdict.txt\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      5\u001B[39m file_path = \u001B[33m\"\u001B[39m\u001B[33mthe-verdict.txt\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[43murllib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43murlretrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/fbcode/platform010/Python3.12.framework/Versions/3.12/lib/python3.12/urllib/request.py:240\u001B[39m, in \u001B[36murlretrieve\u001B[39m\u001B[34m(url, filename, reporthook, data)\u001B[39m\n\u001B[32m    223\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    224\u001B[39m \u001B[33;03mRetrieve a URL into a temporary location on disk.\u001B[39;00m\n\u001B[32m    225\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    236\u001B[39m \u001B[33;03mdata file as well as the resulting HTTPMessage object.\u001B[39;00m\n\u001B[32m    237\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    238\u001B[39m url_type, path = _splittype(url)\n\u001B[32m--> \u001B[39m\u001B[32m240\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m contextlib.closing(\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;28;01mas\u001B[39;00m fp:\n\u001B[32m    241\u001B[39m     headers = fp.info()\n\u001B[32m    243\u001B[39m     \u001B[38;5;66;03m# Just return the local path and the \"headers\" for file://\u001B[39;00m\n\u001B[32m    244\u001B[39m     \u001B[38;5;66;03m# URLs. No sense in performing a copy unless requested.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/fbcode/platform010/Python3.12.framework/Versions/3.12/lib/python3.12/urllib/request.py:215\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[39m\n\u001B[32m    213\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    214\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m215\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/fbcode/platform010/Python3.12.framework/Versions/3.12/lib/python3.12/urllib/request.py:515\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    512\u001B[39m     req = meth(req)\n\u001B[32m    514\u001B[39m sys.audit(\u001B[33m'\u001B[39m\u001B[33murllib.Request\u001B[39m\u001B[33m'\u001B[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001B[32m--> \u001B[39m\u001B[32m515\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    517\u001B[39m \u001B[38;5;66;03m# post-process response\u001B[39;00m\n\u001B[32m    518\u001B[39m meth_name = protocol+\u001B[33m\"\u001B[39m\u001B[33m_response\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/fbcode/platform010/Python3.12.framework/Versions/3.12/lib/python3.12/urllib/request.py:532\u001B[39m, in \u001B[36mOpenerDirector._open\u001B[39m\u001B[34m(self, req, data)\u001B[39m\n\u001B[32m    529\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[32m    531\u001B[39m protocol = req.type\n\u001B[32m--> \u001B[39m\u001B[32m532\u001B[39m result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhandle_open\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\n\u001B[32m    533\u001B[39m \u001B[43m                          \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m_open\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    534\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result:\n\u001B[32m    535\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/fbcode/platform010/Python3.12.framework/Versions/3.12/lib/python3.12/urllib/request.py:492\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    490\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m handler \u001B[38;5;129;01min\u001B[39;00m handlers:\n\u001B[32m    491\u001B[39m     func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m492\u001B[39m     result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    493\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    494\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/fbcode/platform010/Python3.12.framework/Versions/3.12/lib/python3.12/urllib/request.py:1392\u001B[39m, in \u001B[36mHTTPSHandler.https_open\u001B[39m\u001B[34m(self, req)\u001B[39m\n\u001B[32m   1391\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttps_open\u001B[39m(\u001B[38;5;28mself\u001B[39m, req):\n\u001B[32m-> \u001B[39m\u001B[32m1392\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdo_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhttp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mHTTPSConnection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1393\u001B[39m \u001B[43m                        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_context\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/fbcode/platform010/Python3.12.framework/Versions/3.12/lib/python3.12/urllib/request.py:1347\u001B[39m, in \u001B[36mAbstractHTTPHandler.do_open\u001B[39m\u001B[34m(self, http_class, req, **http_conn_args)\u001B[39m\n\u001B[32m   1344\u001B[39m         h.request(req.get_method(), req.selector, req.data, headers,\n\u001B[32m   1345\u001B[39m                   encode_chunked=req.has_header(\u001B[33m'\u001B[39m\u001B[33mTransfer-encoding\u001B[39m\u001B[33m'\u001B[39m))\n\u001B[32m   1346\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1347\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m URLError(err)\n\u001B[32m   1348\u001B[39m     r = h.getresponse()\n\u001B[32m   1349\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "\u001B[31mURLError\u001B[39m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)>"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMtpcgRHepZv",
    "outputId": "51ce84f6-e26c-4f7d-b50d-3e1a4593e270",
    "ExecuteTime": {
     "end_time": "2025-06-23T11:39:23.260270Z",
     "start_time": "2025-06-23T11:39:23.211804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"the-verdict.txt\", \"r\") as fn:\n",
    "    raw_txt = fn.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_txt, 1, 4, 1, False)\n",
    "iter_dataloader = iter(dataloader)\n",
    "first = next(iter_dataloader)\n",
    "first"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Token Embedding"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##Encoding word positions"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The way the previously introduced embedding layer works is that the same token ID always gets mapped to the same vector representation, regardless of where the token ID is positioned in the input sequence.The embedding layer converts a token ID into the same vector representation regardless of where it is located in the input sequence. For example, the token ID 5, whether it’s in the first or fourth position in the token ID input vector, will result in the same embedding vector."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "since the self-attention mechanism of LLMs itself is also position-agnostic, it is helpful to inject additional position information into the LLM."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##Absolute positional embeddings\n",
    "\n",
    "are directly associated with specific positions in a sequence. For each position in the input sequence, a unique embedding is added to the token’s embedding to convey its exact location. For instance, the first token will have a specific positional embedding, the second token another distinct embedding, and so on.\n",
    "\n",
    "\n",
    "Positional embeddings are added to the token embedding vector to create the input embeddings for an LLM. The positional vectors have the same dimension as the original token embeddings. The token embeddings are shown with value 1 for simplicity."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Relative positional embeddings\n",
    "\n",
    "is on the relative position or distance between tokens. This means the model learns the relationships in terms of “how far apart” rather than “at which exact position.” The advantage here is that the model can generalize better to sequences of varying lengths, even if it hasn’t seen such lengths during training."
   ]
  },
  {
   "metadata": {
    "id": "DqazgrfYBYjb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f12ab754-ac13-4ad3-8597-f2ba2082e705"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 256)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "token_embedding_layer"
   ]
  },
  {
   "metadata": {
    "id": "L6JfVy2EXx_S",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a3bb2a2b-c5e9-4408-e510-d998548f68ad"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "# Batch size of 8 with 4 tokens each, 8 * 4 * 256\n",
    "\n",
    "window_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, 8, window_length, window_length, shuffle=False)\n",
    "dataloader_iter = iter(dataloader)\n",
    "inputs, outputs = next(dataloader_iter)\n",
    "inputs.shape\n",
    "# 8 words input , 4 for each word"
   ]
  },
  {
   "metadata": {
    "id": "Oy1A4AyYd7tW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0d42a749-dfbd-4ca8-9768-ddc501b1492a"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "token_embedding = token_embedding_layer(inputs)\n",
    "token_embedding.shape"
   ]
  },
  {
   "metadata": {
    "id": "xdm4MSUod7R2"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# words\n",
    "#-> create a Class  (tensor dataset) return [1. input words ids, 2. predict the next words ids] with encoder your choosed\n",
    "#-> create dataloader with tiktoken encoder.return a tensor dataloader, [input, output(predict next word id)]\n",
    "#-> use torch.nn.Embedding(datasize, output_dim) create a position matrix\n",
    "#-> take the input from tensor dataloader apply to position matrix"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For a GPT model’s absolute embedding approach, we just need to create another embedding layer that has the same embedding dimension as the token_embedding_ layer:"
  },
  {
   "metadata": {
    "id": "gdcuW1Z6ah6X",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3463406d-afe6-4aa6-c194-c8ce08e24ec9"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5664, -0.3090,  1.0387,  ..., -1.4311, -1.0868, -0.3274],\n",
       "        [ 1.4174, -0.0903,  0.5353,  ...,  1.2204, -1.6243, -2.0949],\n",
       "        [ 0.0650, -0.6784,  0.0647,  ..., -0.2292, -0.2698,  0.0311],\n",
       "        [-0.4812,  0.1008,  1.2476,  ...,  0.7328,  0.7332, -0.8771]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "context_length = max_length = 4\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)\n",
    "pos_embeddings\n",
    "# Note: layer is changeable even adopted to other layers\n",
    "# pos_embedding_layer [changeable] -> torch.arange layer [static]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**torch.nn.Embedding(context_length, output_dim)**\n",
    "\n",
    "Creates a learnable lookup table (i.e. embedding matrix) of shape [context_length, output_dim].\n",
    "\n",
    "Each index i from 0 to context_length - 1 maps to a vector of size output_dim.\n",
    "\n",
    "**torch.arange(context_length)**\n",
    "\n",
    "Generates the sequence [0, 1, 2, ..., context_length - 1]. These are the absolute positions in the input sequence.\n",
    "\n",
    "**pos_embedding_layer(...)**\n",
    "\n",
    "Looks up the embedding for each absolute position index.\n",
    "\n",
    "So position 0 always maps to the same vector, position 1 to another, and so on.\n",
    "\n",
    "Result shape: [context_length, output_dim]\n",
    "\n",
    "One embedding vector per absolute position."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SUMMARY"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Raw text is broken into tokens, [words or charactors]\n",
    "2. Tokens into integers -- token IDs\n",
    "3. Handle special tokens: <|unk|> and <|endoftext|>\n",
    "4. => BPE tokenizer -- tiktoken.get_encoding(\"gpt2\")\n",
    "5. Sliding window approach on tokenized data, input <-> predicted next word\n",
    "6. Embedding layers in PyTorch function as a lookup operation, retrieving vectors corresponding to token IDs.\n",
    "7. OpenAI’s GPT models utilize absolute positional embeddings, which are added to the token embedding vectors and are optimized during the model training."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Start Token Embedding"
  },
  {
   "metadata": {
    "id": "lonRWe96ahwe",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e5d169ac-afc3-457e-8d2a-186cc9c19fd7"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n"
     ]
    }
   ],
   "execution_count": null,
   "source": "pip install tiktoken"
  },
  {
   "metadata": {
    "id": "KuJ5GeK4uVNN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b443788e-8476-4f9a-82ce-d050624a750d"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2953, 428, 966, 11, 345]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "# 4. => BPE tokenizer -- tiktoken.get_encoding(\"gpt2\")\n",
    "import tiktoken\n",
    "raw_text = \"At this point, you know how to prepare the input text for training LLMs by splitting text into individual word and subword tokens, which can be encoded into vector representations, embeddings, for the LLM.\"\n",
    "BPE_tokenizer_v1 = tiktoken.get_encoding(\"gpt2\")\n",
    "txt_encoding = BPE_tokenizer_v1.encode(raw_text)\n",
    "txt_encoding[:5]"
   ]
  },
  {
   "metadata": {
    "id": "ks9aQWNWuVD4"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#5. Sliding window approach on tokenized data, input <-> predicted next word\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPT_Dataset(Dataset):\n",
    "  def __init__(self, txt, tokenizer, window_size, steps):\n",
    "    self.input_ids = []\n",
    "    self.output_ids = []\n",
    "\n",
    "    word_nums = tokenizer.encode(txt)\n",
    "    for i in range(0, len(word_nums) - window_size, steps):\n",
    "      tensor_input = torch.tensor(word_nums[i : i + window_size])\n",
    "      tensor_output = torch.tensor(word_nums[i + 1 : i + window_size + 1])\n",
    "      self.input_ids.append(tensor_input)\n",
    "      self.output_ids.append(tensor_output)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.output_ids[idx]\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "id": "fii3WHmnuVAu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e8d41f27-7fc1-43fb-e1b0-080aa2bb9d29"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "#Embedding layers in PyTorch function as a lookup operation, retrieving vectors corresponding to token IDs\n",
    "def customize_dataloader(txt, batch_size=4, window_size=256, steps=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "  dataset = GPT_Dataset(txt, tokenizer, window_size, steps)\n",
    "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "  return dataloader\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\") as fn:\n",
    "    raw_txt = fn.read()\n",
    "\n",
    "window_size = 4\n",
    "dataloader = customize_dataloader(raw_text, 8, window_size, window_size, shuffle=False)\n",
    "inputs, outputs = next(iter(dataloader))\n",
    "inputs.shape\n"
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d66f9174-b784-4ce7-dc31-cddf3fa597c8",
    "id": "mVf2f27-onww"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "# OpenAI’s GPT models utilize absolute positional embeddings, which are added to the token embedding vectors and are optimized during the model training.\n",
    "\n",
    "input_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(input_size, output_dim)\n",
    "token_embedding = token_embedding_layer(inputs)\n",
    "token_embedding.shape"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Start Positional Embeddings"
  },
  {
   "metadata": {
    "id": "h4kKiuyNuU9j",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "53f74c2d-9b13-462b-e7f5-8c8f66467288"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "context_length = windew_size = 4\n",
    "position_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "position_embedding = position_embedding_layer(torch.arange(context_length))\n",
    "position_embedding.shape"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Start Input Embedding"
  },
  {
   "metadata": {
    "id": "DaXr68cVuU6X",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "99d927a1-fa46-4197-877f-be9aaf967082"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null,
   "source": [
    "input_embedding = token_embedding + position_embedding\n",
    "input_embedding.shape"
   ]
  }
 ]
}
